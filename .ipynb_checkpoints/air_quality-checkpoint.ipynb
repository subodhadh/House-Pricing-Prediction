{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fff2eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c43c3dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>TM</th>\n",
       "      <th>Tm</th>\n",
       "      <th>SLP</th>\n",
       "      <th>H</th>\n",
       "      <th>VV</th>\n",
       "      <th>V</th>\n",
       "      <th>VM</th>\n",
       "      <th>PM 2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>9.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>9.4</td>\n",
       "      <td>219.720833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>12.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1018.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>11.1</td>\n",
       "      <td>182.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1019.4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>11.1</td>\n",
       "      <td>154.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.6</td>\n",
       "      <td>15.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1018.7</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8.1</td>\n",
       "      <td>20.6</td>\n",
       "      <td>223.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.4</td>\n",
       "      <td>20.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1017.3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>8.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>200.645833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T    TM   Tm     SLP     H   VV    V    VM      PM 2.5\n",
       "0   7.4   9.8  4.8  1017.6  93.0  0.5  4.3   9.4  219.720833\n",
       "1   7.8  12.7  4.4  1018.5  87.0  0.6  4.4  11.1  182.187500\n",
       "2   6.7  13.4  2.4  1019.4  82.0  0.6  4.8  11.1  154.037500\n",
       "3   8.6  15.5  3.3  1018.7  72.0  0.8  8.1  20.6  223.208333\n",
       "4  12.4  20.9  4.4  1017.3  61.0  1.3  8.7  22.2  200.645833"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data=pd.read_csv(\"Real_Combine.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f849dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dependent and independent variable\n",
    "x=data.iloc[:,1:8]#independent variable\n",
    "y=data.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ca139ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TM    Tm     SLP     H   VV     V    VM\n",
      "784   38.0  23.2  1002.6  30.0  3.1  18.7  35.2\n",
      "741   15.7   7.5  1018.9  86.0  0.8   6.1  11.1\n",
      "747   22.4   8.4  1017.6  66.0  2.4   7.2  16.5\n",
      "986   39.0  27.4   996.6  50.0  2.4   9.4  27.8\n",
      "479   35.5  25.4  1001.9  63.0  1.9   5.4  11.1\n",
      "...    ...   ...     ...   ...  ...   ...   ...\n",
      "655   35.0  28.3  1001.3  72.0  2.1   2.8   9.4\n",
      "1087  23.0   6.6  1016.9  61.0  1.0   9.4  20.6\n",
      "869   35.0  24.7  1007.3  62.0  2.3   7.6  18.3\n",
      "665   34.0  25.5  1000.2  80.0  1.4   3.3  11.1\n",
      "674   31.0  25.2  1001.3  88.0  2.4   4.6   7.6\n",
      "\n",
      "[219 rows x 7 columns]         TM    Tm     SLP     H   VV     V    VM\n",
      "863   37.2  25.3  1003.0  59.0  3.1   1.7   7.6\n",
      "422   32.5  20.0  1012.8  68.0  2.6   3.0  11.1\n",
      "342   31.2  15.0  1013.4  58.0  1.1   4.1  14.8\n",
      "188   20.7   4.4  1017.9  78.0  0.8   1.9   7.6\n",
      "934   26.0  11.5  1014.7  65.0  2.3   5.7  14.8\n",
      "...    ...   ...     ...   ...  ...   ...   ...\n",
      "1033  35.0  26.8  1004.1  76.0  2.6   6.1  14.8\n",
      "763   32.4  15.2  1015.0  57.0  1.9   1.7   9.4\n",
      "835   37.2  29.0   998.4  61.0  2.6  11.5  22.2\n",
      "559   19.0   8.9  1015.8  91.0  0.5   4.6   9.4\n",
      "684   34.8  25.6  1004.5  66.0  2.6   3.3  11.1\n",
      "\n",
      "[874 rows x 7 columns] 784      46.916667\n",
      "741     107.625000\n",
      "747     332.708333\n",
      "986     110.416667\n",
      "479      22.708333\n",
      "           ...    \n",
      "655      38.250000\n",
      "1087    284.166667\n",
      "869      60.541667\n",
      "665      26.833333\n",
      "674      30.625000\n",
      "Name: PM 2.5, Length: 219, dtype: float64 863      24.875000\n",
      "422      52.625000\n",
      "342     196.541667\n",
      "188     264.916667\n",
      "934     132.208333\n",
      "           ...    \n",
      "1033     64.916667\n",
      "763       1.833333\n",
      "835      52.458333\n",
      "559     108.000000\n",
      "684      41.500000\n",
      "Name: PM 2.5, Length: 874, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#train_test_split\n",
    "x_train , x_test , y_train , y_test=train_test_split(x , y , test_size=0.2 , random_state=0)\n",
    "print(x_test , x_train , y_test , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1264c7e-d034-4bc8-a773-8c92695d5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(np.isnan(y_train), np.nanmean(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccc91bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Convert y to NumPy array and make 2D\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "# Feature scaler\n",
    "X_scaler = StandardScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(x_train)\n",
    "X_test_scaled = X_scaler.transform(x_test)\n",
    "\n",
    "# Target scaler\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "# Check NaNs or Infs\n",
    "print(np.isnan(X_train_scaled).sum(), np.isnan(y_train_scaled).sum())\n",
    "print(np.isinf(X_train_scaled).sum(), np.isinf(y_train_scaled).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44decb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a model\n",
    "\n",
    "def build_model(hp):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=hp.Int('units_input', min_value=32 , max_value=256 , step=32),\n",
    "                     activation=hp.Choice('activation _input',['relu' , 'tanh']),\n",
    "                     input_dim=x_train_scaled.shape[1]))\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,3)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32 , max_value=256 , step=32),\n",
    "                        activation=hp.Choice(f'activation{i}',['relu' , 'tanh'])))\n",
    "    model.add(Dense(units=1 , activation='linear'))\n",
    "    #compiling model\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-3, 1e-4])) , loss='mean_squared_error' ,metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88007676",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Failed to delete a file: \\\\?\\C:\\my coding journey\\deep_learning\\my_dir\\air_quality_ann/trial_03/checkpoint.weights.h5; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionDeniedError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tuner=\u001b[43mRandomSearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmy_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mair_quality_ann\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m tuner.search(x_train_scaled , y_train_scaled, validation_split=\u001b[32m0.1\u001b[39m, epochs=\u001b[32m50\u001b[39m, batch_size=\u001b[32m16\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\my coding journey\\deep_learning\\deep\\Lib\\site-packages\\keras_tuner\\src\\tuners\\randomsearch.py:174\u001b[39m, in \u001b[36mRandomSearch.__init__\u001b[39m\u001b[34m(self, hypermodel, objective, max_trials, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mself\u001b[39m.seed = seed\n\u001b[32m    164\u001b[39m oracle = RandomSearchOracle(\n\u001b[32m    165\u001b[39m     objective=objective,\n\u001b[32m    166\u001b[39m     max_trials=max_trials,\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     max_consecutive_failed_trials=max_consecutive_failed_trials,\n\u001b[32m    173\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\my coding journey\\deep_learning\\deep\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:122\u001b[39m, in \u001b[36mTuner.__init__\u001b[39m\u001b[34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hypermodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.run_trial \u001b[38;5;129;01mis\u001b[39;00m Tuner.run_trial:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    116\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`hypermodel` if the user defines the search space in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing a `HyperModel` instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43moracle\u001b[49m\u001b[43m=\u001b[49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m.max_model_size = max_model_size\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer = optimizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\my coding journey\\deep_learning\\deep\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:121\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.oracle._set_project_dir(\u001b[38;5;28mself\u001b[39m.directory, \u001b[38;5;28mself\u001b[39m.project_name)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;129;01mand\u001b[39;00m backend.io.exists(\u001b[38;5;28mself\u001b[39m.project_dir):\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproject_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# To support tuning distribution.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mself\u001b[39m.tuner_id = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mKERASTUNER_TUNER_ID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtuner0\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\my coding journey\\deep_learning\\deep\\Lib\\site-packages\\keras_tuner\\src\\backend\\io.py:37\u001b[39m, in \u001b[36mrmtree\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m shutil.rmtree(path)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\my coding journey\\deep_learning\\deep\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:676\u001b[39m, in \u001b[36mdelete_recursively_v2\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mio.gfile.rmtree\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdelete_recursively_v2\u001b[39m(path):\n\u001b[32m    668\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Deletes everything under path recursively.\u001b[39;00m\n\u001b[32m    669\u001b[39m \n\u001b[32m    670\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    674\u001b[39m \u001b[33;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDeleteRecursively\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPermissionDeniedError\u001b[39m: Failed to delete a file: \\\\?\\C:\\my coding journey\\deep_learning\\my_dir\\air_quality_ann/trial_03/checkpoint.weights.h5; Permission denied"
     ]
    }
   ],
   "source": [
    "tuner=RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=2,\n",
    "    directory='my_dir',\n",
    "    project_name='air_quality_ann',\n",
    "    overwrite=True\n",
    ")\n",
    "tuner.search(x_train_scaled , y_train_scaled, validation_split=0.1, epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaee996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
